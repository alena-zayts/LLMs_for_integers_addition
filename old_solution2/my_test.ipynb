{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/first_results/epoch=11-val_exact_match=1.0000.ckpt')"
      ],
      "metadata": {
        "id": "pkplcf1eyL-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning==1.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA_jgd4LNdTB",
        "outputId": "c3e41d03-f036-4912-a2a4-104f573d2130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning==1.9.0\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (2023.4.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (1.22.4)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.0) (6.0)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch_lightning==1.9.0) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch_lightning==1.9.0) (3.25.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (23.1.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->pytorch_lightning==1.9.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch_lightning==1.9.0) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-1.9.0 torchmetrics-0.11.4 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0YSnqLRNxiH",
        "outputId": "9dbe2bfc-5c4b-42dd-db8e-ec69dcccb87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.1\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evalute T5 on arithmetic problems.\n",
        "SEED = 1\n",
        "\n",
        "# 1/12 - 12 = TRAIN_SIZE / TRAIN_BATCh + valid_size / valid_batch\n",
        "\n",
        "NUM_WORKERS = 4\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "GAMMA = 1.0  # 0.1\n",
        "\n",
        "STEP_SIZE = 1000\n",
        "OPTIMIZER_NAME = 'AdamW'\n",
        "\n",
        "MODEL_NAME = 't5-base'  # t5-small, t5-base\n",
        "MIN_DIGITS_TRAIN = 2\n",
        "MAX_DIGITS_TRAIN = 15\n",
        "MIN_DIGITS_TEST = 2\n",
        "MAX_DIGITS_TEST = 15\n",
        "OUTPUT_DIR = 'first_results'\n",
        "\n",
        "TRAIN_SIZE = 100000\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VAL_SIZE = 10000\n",
        "VAL_BATCH_SIZE = 32\n",
        "TEST_SIZE = 10000\n",
        "\n",
        "MAX_SEQ_LEN = 512\n",
        "CHECK_VAL_EVERY_N_EPOCH = 2\n",
        "MAX_EPOCHS = 20\n",
        "GPUS = 1"
      ],
      "metadata": {
        "id": "zY8xLED_yhBd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "f9tsbOZuMxT8"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def compute_exact_match(predicted_answer, correct_answer) -> bool:\n",
        "    predicted_answer = predicted_answer.strip().lower()\n",
        "    correct_answer = correct_answer.strip().lower()\n",
        "    return predicted_answer == correct_answer\n",
        "\n",
        "\n",
        "def convert_to_10ebased(number: str) -> str:\n",
        "    signal = None\n",
        "    if number[0] == '-':\n",
        "        signal = '-'\n",
        "        number = number[1:]\n",
        "\n",
        "    output = []\n",
        "    for i, digit in enumerate(number[::-1]):\n",
        "        output.append('10e' + str(i))\n",
        "        output.append(digit)\n",
        "\n",
        "    if signal:\n",
        "        output.append(signal)\n",
        "\n",
        "    # as we want it to _not_ be inverted, then we invert it.\n",
        "    output = output[::-1]\n",
        "\n",
        "    return ' '.join(output)\n",
        "\n",
        "\n",
        "\n",
        "def translate_task(a_int: int, b_int: int):\n",
        "    result_int = a_int + b_int\n",
        "\n",
        "    a_str = convert_to_10ebased(str(a_int))\n",
        "    b_str = convert_to_10ebased(str(b_int))\n",
        "    result_str = convert_to_10ebased(str(result_int))\n",
        "\n",
        "    question = f'What is {a_str} plus {b_str}?'\n",
        "    return {\n",
        "        'a_int': a_int,\n",
        "        'b_int': b_int,\n",
        "        'expected_result_int': result_int,\n",
        "\n",
        "        'a_str': a_str,\n",
        "        'b_str': b_str,\n",
        "        'expected_result_str': result_str,\n",
        "\n",
        "        'question': question,\n",
        "    }\n",
        "\n",
        "class T5Finetuner(pl.LightningModule):\n",
        "    def __init__(self, train_dataloader, val_dataloader, test_dataloader):\n",
        "        super(T5Finetuner, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        self._train_dataloader = train_dataloader\n",
        "        self._val_dataloader = val_dataloader\n",
        "        self._test_dataloader = test_dataloader\n",
        "\n",
        "    def prepare_batch(self, questions: List[str], answers: List[str]) -> List[str]:\n",
        "\n",
        "        input_dict = self.tokenizer.batch_encode_plus(\n",
        "            list(questions), padding=True, truncation=False, return_tensors='pt')\n",
        "\n",
        "        labels = self.tokenizer.batch_encode_plus(\n",
        "            list(answers), padding=True, truncation=False, return_tensors='pt')['input_ids']\n",
        "\n",
        "        assert input_dict['input_ids'].shape[1] < MAX_SEQ_LEN\n",
        "        assert labels.shape[1] < MAX_SEQ_LEN\n",
        "\n",
        "        input_ids = input_dict['input_ids'].to(self.model.device)\n",
        "        attention_mask = input_dict['attention_mask'].to(self.model.device)\n",
        "        labels = labels.to(self.model.device)\n",
        "\n",
        "        return input_ids, attention_mask, labels\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.model(**kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        questions, correct_answers = batch\n",
        "\n",
        "        # Log every power of two.\n",
        "        if batch_nb & (batch_nb - 1) == 0:\n",
        "            print(questions[0])\n",
        "            print(correct_answers[0])\n",
        "\n",
        "        input_ids, attention_mask, labels = self.prepare_batch(questions=questions, answers=correct_answers)\n",
        "\n",
        "        loss = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)[0]\n",
        "\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "      \n",
        "    def predict(self, question):\n",
        "        input_ids, attention_mask, _ = self.prepare_batch(questions=[question], answers=[question])\n",
        "        batch_outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=False,\n",
        "                                            max_length=MAX_SEQ_LEN)\n",
        "\n",
        "        predicted_answers = [\n",
        "            self.tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in batch_outputs]\n",
        "          \n",
        "        return predicted_answers[0]\n",
        "\n",
        "\n",
        "    def inference_step(self, batch, batch_nb: int):\n",
        "        questions, correct_answers = batch\n",
        "\n",
        "        input_ids, attention_mask, _ = self.prepare_batch(questions=questions, answers=correct_answers)\n",
        "        batch_outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=False,\n",
        "                                            max_length=MAX_SEQ_LEN)\n",
        "\n",
        "        predicted_answers = [\n",
        "            self.tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in batch_outputs]\n",
        "\n",
        "        exact_matches = [\n",
        "            compute_exact_match(predicted_answer=predicted_answer, correct_answer=correct_answer)\n",
        "            for predicted_answer, correct_answer in zip(predicted_answers, correct_answers)]\n",
        "\n",
        "        # Log every power of two.\n",
        "        if batch_nb & (batch_nb - 1) == 0:\n",
        "            print('\\nQuestion:', questions[0])\n",
        "            print('Correct:  ', correct_answers[0])\n",
        "            print('Predicted:', predicted_answers[0].encode('utf-8'))\n",
        "            print('Exact?', exact_matches[0])\n",
        "\n",
        "        metrics = {'exact_matches': exact_matches}\n",
        "        return metrics\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        return self.inference_step(batch, batch_nb)\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        return self.inference_step(batch, batch_nb)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        print('QQ: in validation_epoch_end')\n",
        "        exact_matches = []\n",
        "        for x in outputs:\n",
        "            exact_matches.extend(x['exact_matches'])\n",
        "        exact_match = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "        metrics = {'val_exact_match': exact_match}\n",
        "\n",
        "        output = metrics.copy()\n",
        "        output['progress_bar'] = metrics\n",
        "\n",
        "        # added\n",
        "        self.log('val_exact_match', exact_match, prog_bar=True)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        exact_matches = []\n",
        "        for x in outputs:\n",
        "            exact_matches.extend(x['exact_matches'])\n",
        "        exact_match = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "        metrics = {'test_exact_match': exact_match}\n",
        "        print('test_exact_match', exact_match)\n",
        "\n",
        "        output = metrics.copy()\n",
        "        output['progress_bar'] = metrics\n",
        "        self.log('test_exact_match', exact_match, prog_bar=True)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self._train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self._val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self._test_dataloader\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        optimizer = getattr(torch.optim, OPTIMIZER_NAME)\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0\n",
        "            },\n",
        "        ]\n",
        "        optimizer = optimizer(optimizer_grouped_parameters, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        print(f'=> Using {OPTIMIZER_NAME} optimizer')\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE,\n",
        "                                                    gamma=GAMMA)\n",
        "        print(f'=> Using StepLR (step_size = {STEP_SIZE}, gamma = {GAMMA}) scheduler')\n",
        "\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = self.get_optimizer()\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, n_examples: int, min_digits: int, max_digits: int):\n",
        "\n",
        "        self.max_digits = max_digits\n",
        "\n",
        "        # if balance:\n",
        "        self.examples = []\n",
        "        for _ in range(n_examples):\n",
        "            example = []\n",
        "            for _ in range(2):\n",
        "                max_digits_i = random.randint(min_digits, max_digits)\n",
        "                min_number = int((max_digits_i - 1) * '9') + 1\n",
        "                max_number = int(max_digits_i * '9')\n",
        "                example.append(random.randint(min_number, max_number))\n",
        "            self.examples.append(example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        first_term, second_term = self.examples[idx]\n",
        "        translated_task = translate_task(first_term, second_term)\n",
        "        \n",
        "\n",
        "        return translated_task['question'], translated_task['expected_result_str']\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    random.seed(SEED)\n",
        "    pl.seed_everything(SEED)\n",
        "\n",
        "    dataset_train = MyDataset(n_examples=TRAIN_SIZE, min_digits=MIN_DIGITS_TRAIN, max_digits=MAX_DIGITS_TRAIN)\n",
        "    dataset_val = MyDataset(n_examples=VAL_SIZE, min_digits=MIN_DIGITS_TRAIN, max_digits=MAX_DIGITS_TRAIN)\n",
        "    dataset_test = MyDataset(n_examples=TEST_SIZE, min_digits=MIN_DIGITS_TEST, max_digits=MAX_DIGITS_TEST)\n",
        "\n",
        "    train_dataloader = DataLoader(dataset_train, batch_size=TRAIN_BATCH_SIZE,\n",
        "                                  shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_dataloader = DataLoader(dataset_val, batch_size=VAL_BATCH_SIZE,\n",
        "                                shuffle=False, num_workers=NUM_WORKERS)\n",
        "    test_dataloader = DataLoader(dataset_test, batch_size=VAL_BATCH_SIZE,\n",
        "                                 shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=OUTPUT_DIR, filename='{epoch}-{val_exact_match:.4f}',\n",
        "        verbose=False, save_last=True, save_top_k=2, mode='max', monitor='val_exact_match',\n",
        "        save_weights_only=False, every_n_epochs=CHECK_VAL_EVERY_N_EPOCH,\n",
        "        # save_on_train_epoch_end=True\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        precision=32,\n",
        "        callbacks=[checkpoint_callback],\n",
        "        max_epochs=MAX_EPOCHS,\n",
        "        check_val_every_n_epoch=CHECK_VAL_EVERY_N_EPOCH,\n",
        "        accumulate_grad_batches=32,\n",
        "        gradient_clip_val=1.0,\n",
        "        amp_level='O0',\n",
        "        amp_backend='apex',\n",
        "        gpus=GPUS)\n",
        "\n",
        "    model = T5Finetuner(train_dataloader=train_dataloader,\n",
        "                        val_dataloader=val_dataloader,\n",
        "                        test_dataloader=test_dataloader)\n",
        "\n",
        "    trainer.fit(model)\n",
        "  \n",
        "# train()\n",
        "\n",
        "# checkpoint_path = glob.glob(os.path.join(OUTPUT_DIR, '*.ckpt'))[0]\n",
        "# checkpoint_path = '/content/first_results/epoch=11-val_exact_match=1.0000.ckpt'\n",
        "# model = T5Finetuner.load_from_checkpoint(checkpoint_path,\n",
        "#                                          train_dataloader=train_dataloader,\n",
        "#                                          val_dataloader=val_dataloader,\n",
        "#                                          test_dataloader=test_dataloader)\n",
        "\n",
        "# results = trainer.test(model)\n",
        "\n",
        "# output = {'seed': SEED,\n",
        "#           'max_digits_train': MAX_DIGITS_TRAIN,\n",
        "#           'max_digits_test': MAX_DIGITS_TEST,\n",
        "#           'test_exact_match_': results[0]}\n",
        "\n",
        "# with open(os.path.join(OUTPUT_DIR, 'results.json'), 'w') as fout:\n",
        "#     json.dump(output, fout)\n",
        "\n",
        "# print('Done!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class Solver2():\n",
        "    def __init__(self):\n",
        "        checkpoint_path = '/content/first_results/epoch=11-val_exact_match=1.0000.ckpt'\n",
        "\n",
        "        self.model = T5Finetuner.load_from_checkpoint(checkpoint_path, \n",
        "                                                      train_dataloader=None, \n",
        "                                                      val_dataloader=None, \n",
        "                                                      test_dataloader=None)\n",
        "        \n",
        "\n",
        "    def calc_sum(self, a: int, b: int) -> Tuple[int, dict]:\n",
        "        translated_task = translate_task(a, b)\n",
        "        model_answer = self.model.predict(translated_task['question'])\n",
        "        return model_answer, translated_task\n",
        "\n",
        "        # real_model_answer = full_model_answer[len(question):]\n",
        "        # code_model_answer = real_model_answer.split(QUESTION_START_MARKER)[0]\n",
        "\n",
        "        # extra_tab_len = 4\n",
        "        # code_model_answer = code_model_answer.split('\\n')\n",
        "        # for i in range(len(code_model_answer)):\n",
        "        #     if len(code_model_answer[i]) > extra_tab_len:\n",
        "        #         code_model_answer[i] = code_model_answer[i][extra_tab_len:]\n",
        "\n",
        "        # # do not change order\n",
        "        # answer_int = self.execute(code_model_answer)\n",
        "        # code_model_answer = '\\n'.join(code_model_answer)\n",
        "\n",
        "        # meta_info = {\n",
        "        #     'question': question,\n",
        "        #     'full_model_answer': full_model_answer,\n",
        "        #     'code_model_answer': code_model_answer,\n",
        "        #     'answer_int': answer_int\n",
        "        # }\n",
        "\n",
        "        # return answer_int, meta_info\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    solver = Solver2()\n",
        "    a = 2\n",
        "    b = 3\n",
        "    expected = a + b\n",
        "    answer_int, meta_info = solver.calc_sum(a, b)\n",
        "\n",
        "    print(meta_info)\n",
        "    print(answer_int)\n",
        "    print(expected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAJ2deWoMawP",
        "outputId": "2fc7cd76-0975-41cd-aade-37bfa90293b2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a_int': 2, 'b_int': 3, 'expected_result_int': 5, 'a_str': '2 10e0', 'b_str': '3 10e0', 'expected_result_str': '5 10e0', 'question': 'What is 2 10e0 plus 3 10e0?'}\n",
            "5 10e0\n",
            "5\n"
          ]
        }
      ]
    }
  ]
}